{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joywinbennis/Langchain/.venv/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x12858c210>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Deep_learning\")\n",
    "docs = loader.load()\n",
    "document = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)[:30]\n",
    "db = FAISS.from_documents(document, OllamaEmbeddings(model=\"mistral\"))\n",
    "retrevier = db.as_retriever()\n",
    "retrevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep learning - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\n Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1Definition\\n\\n\\n\\n\\n\\n\\n\\n2Overview\\n\\n\\n\\n\\n\\n\\n\\n3Interpretations\\n\\n\\n\\n\\n\\n\\n\\n4History\\n\\n\\n\\nToggle History subsection\\n\\n\\n\\n\\n\\n4.1Deep learning revolution\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5Neural networks\\n\\n\\n\\nToggle Neural networks subsection\\n\\n\\n\\n\\n\\n5.1Deep neural networks\\n\\n\\n\\n\\n\\n5.1.1Challenges\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6Hardware\\n\\n\\n\\n\\n\\n\\n\\n7Applications\\n\\n\\n\\nToggle Applications subsection\\n\\n\\n\\n\\n\\n7.1Automatic speech recognition\\n\\n\\n\\n\\n\\n\\n\\n7.2Image recognition', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='5.1Deep neural networks\\n\\n\\n\\n\\n\\n5.1.1Challenges\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6Hardware\\n\\n\\n\\n\\n\\n\\n\\n7Applications\\n\\n\\n\\nToggle Applications subsection\\n\\n\\n\\n\\n\\n7.1Automatic speech recognition\\n\\n\\n\\n\\n\\n\\n\\n7.2Image recognition\\n\\n\\n\\n\\n\\n\\n\\n7.3Visual art processing\\n\\n\\n\\n\\n\\n\\n\\n7.4Natural language processing\\n\\n\\n\\n\\n\\n\\n\\n7.5Drug discovery and toxicology\\n\\n\\n\\n\\n\\n\\n\\n7.6Customer relationship management\\n\\n\\n\\n\\n\\n\\n\\n7.7Recommendation systems\\n\\n\\n\\n\\n\\n\\n\\n7.8Bioinformatics\\n\\n\\n\\n\\n\\n\\n\\n7.9Deep Neural Network Estimations\\n\\n\\n\\n\\n\\n\\n\\n7.10Medical image analysis\\n\\n\\n\\n\\n\\n\\n\\n7.11Mobile advertising\\n\\n\\n\\n\\n\\n\\n\\n7.12Image restoration\\n\\n\\n\\n\\n\\n\\n\\n7.13Financial fraud detection\\n\\n\\n\\n\\n\\n\\n\\n7.14Materials science\\n\\n\\n\\n\\n\\n\\n\\n7.15Military\\n\\n\\n\\n\\n\\n\\n\\n7.16Partial differential equations\\n\\n\\n\\n\\n\\n\\n\\n7.17Image reconstruction\\n\\n\\n\\n\\n\\n\\n\\n7.18Epigenetic clock\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n8Relation to human cognitive and brain development\\n\\n\\n\\n\\n\\n\\n\\n9Commercial activity\\n\\n\\n\\n\\n\\n\\n\\n10Criticism and comment\\n\\n\\n\\nToggle Criticism and comment subsection\\n\\n\\n\\n\\n\\n10.1Theory\\n\\n\\n\\n\\n\\n\\n\\n10.2Errors\\n\\n\\n\\n\\n\\n\\n\\n10.3Cyber threat\\n\\n\\n\\n\\n\\n\\n\\n10.4Data collection ethics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n11See also', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='10Criticism and comment\\n\\n\\n\\nToggle Criticism and comment subsection\\n\\n\\n\\n\\n\\n10.1Theory\\n\\n\\n\\n\\n\\n\\n\\n10.2Errors\\n\\n\\n\\n\\n\\n\\n\\n10.3Cyber threat\\n\\n\\n\\n\\n\\n\\n\\n10.4Data collection ethics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n11See also\\n\\n\\n\\n\\n\\n\\n\\n12References\\n\\n\\n\\n\\n\\n\\n\\n13Further reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nDeep learning\\n\\n\\n\\n51 languages\\n\\n\\n\\n\\nAfrikaansالعربيةAzərbaycancaবাংলাBân-lâm-gúБългарскиBosanskiCatalàČeštinaDanskDeutschEestiEspañolEuskaraفارسیFrançaisGaeilgeGalego한국어ՀայերենBahasa IndonesiaItalianoעבריתമലയാളംBahasa MelayuМонголNederlands日本語Norsk bokmålOccitanپښتوPolskiPortuguêsRomânăRuna SimiРусскийShqipසිංහලSimple EnglishSlovenščinaکوردیСрпски / srpskiSuomiSvenskaதமிழ்ไทยTürkçeУкраїнськаTiếng Việt粵語中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Edit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikimedia Commons\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nBranch of machine learning\\nFor the TV series episode, see Deep Learning (South Park).\\nRepresenting images on multiple layers of abstraction in deep learning[1]\\nPart of a series onArtificial intelligence\\nMajor goals\\nArtificial general intelligence\\nRecursive self-improvement\\nPlanning\\nComputer vision\\nGeneral game playing\\nKnowledge reasoning\\nMachine learning\\nNatural language processing\\nRobotics\\nAI safety', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Approaches\\nSymbolic\\nDeep learning\\nBayesian networks\\nEvolutionary algorithms\\nSituated approach\\nHybrid intelligent systems\\nSystems integration\\n\\nApplications\\nProjects\\nDeepfake\\nMachine translation\\nGenerative AI\\nArt\\nAudio\\nMusic\\nHealthcare\\nMental health\\nGovernment\\nIndustry\\nEarth sciences\\nBioinformatics\\nPhysics\\n\\nPhilosophy\\nChinese room\\nFriendly AI\\nControl problem/Takeover\\nEthics\\nExistential risk\\nTuring test\\nRegulation\\n\\nHistory\\nTimeline\\nProgress\\nAI winter\\nAI boom\\nAI era', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Glossary\\nGlossary\\nvte\\nDeep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\\nDeep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7] ANNs are generally seen as low quality models for brain function.[8]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Definition[edit]\\nDeep learning is a class of machine learning algorithms that[9]:\\u200a199–200\\u200a uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human, such as digits, letters, or faces.\\nFrom another angle to view deep learning, deep learning refers to \"computer-simulate\" or \"automate\" human learning processes from a source (e.g., an image of dogs) to a learned object (dogs). Therefore, a notion coined as \"deeper\" learning or \"deepest\" learning[10] makes sense. The deepest learning refers to the fully automatic learning from a source to a final learned object. A deeper learning thus refers to a mixed learning process: a human learning process from a source to a learned semi-object, followed by a computer learning process from the human learned semi-object to a final learned object.', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Overview[edit]\\nMost modern deep learning models are based on multi-layered artificial neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[11]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[12][13]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[14] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[15] Beyond that, more layers do not add to the function approximator ability of', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[15] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Deep learning architectures can be constructed with a greedy layer-by-layer method.[16] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[12]\\nFor supervised learning tasks, deep learning methods enable elimination of feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[12][17]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Machine learning models are now adept at identifying complex patterns in financial market data. Due to the benefits of artificial intelligence, investors are increasingly utilizing deep learning techniques to forecast and analyze trends in stock and foreign exchange markets.[18]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"Interpretations[edit]\\nDeep neural networks are generally interpreted in terms of the universal approximation theorem[19][20][21][22][23] or probabilistic inference.[24][9][12][14][25]\\nThe classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[19][20][21][22] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[19] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[20] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[26][27]\", metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[23] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='The probabilistic interpretation[25] derives from the field of machine learning. It features inference,[9][11][12][14][17][25] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[25] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[28]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"History[edit]\\nThere are two types of artificial neural network (ANN): feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created and analyzed the Ising model[29] which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.[30][31] His learning RNN was popularised by John Hopfield in 1982.[32] RNNs have become central for speech recognition and language processing.\", metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"Charles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today,[33] referring to Rosenblatt's 1962 book[34] which introduced multilayer perceptron (MLP) with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. It also introduced variants, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).[34]:\\u200asection 16\\u200a In addition, term deep learning was proposed in 1986 by Rina Dechter[35] although the history of its appearance is apparently more complicated.[36]\\nThe first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967.[37] A 1971 paper described a deep network with eight layers trained by the group method of data handling.[38]\", metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"The first deep learning multilayer perceptron trained by stochastic gradient descent[39] was published in 1967 by Shun'ichi Amari.[40][31] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.[31] In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples, but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical, and proposed using fixed random early layers as an input hash for a single modifiable layer.[41]  Instead, subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\", metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='In 1970, Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions.[42][43][44] This became known as backpropagation.[14] It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[45] to networks of differentiable nodes.[31] \\nThe terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[34][31] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation[46] already in 1960 in the context of control theory.[31] In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.[47][48][31] In 1985, David E. Rumelhart et al. published an experimental analysis of the technique.[49]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[50] In 1969, he also introduced the ReLU (rectified linear unit) activation function.[26][31] The rectifier has become the most popular activation function for CNNs and deep learning in general.[51] CNNs have become an essential tool for computer vision.\\nThe term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[35] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[52][53]\\nIn 1988, Wei Zhang et al. applied the backpropagation algorithm', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='In 1988, Wei Zhang et al. applied the backpropagation algorithm \\nto a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.[54][55] \\nIn 1989, Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[56] Subsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991[57] and breast cancer detection in mammograms in 1994.[58] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al.,[59] that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning.[60] It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network.[60][31] In 1993, a chunker solved a deep learning task whose depth exceeded 1000.[61]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='In 1992, Jürgen Schmidhuber also published an alternative to RNNs[62] which is now called a linear Transformer or a  Transformer with linearized self-attention[63][64][31] (save for a normalization operator). It learns internal spotlights of attention:[65] a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention).[63] This fast weight attention mapping is applied to a query pattern.\\nThe modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper \"Attention Is All You Need\".[66] \\nIt combines this with a softmax operator and a projection matrix.[31]\\nTransformers have increasingly become the model of choice for natural language processing.[67] Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.[68]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network\\'s gain is the other network\\'s loss.[69][70][71] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al.[72] Here the environmental reaction is 1 or 0 depending on whether the first network\\'s output is in a given set. This can be used to create realistic deepfakes.[73] Excellent image quality is achieved by Nvidia\\'s StyleGAN (2018)[74] based on the Progressive GAN by Tero Karras et al.[75] Here the GAN generator is grown from small to large scale in a pyramidal fashion.', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='Sepp Hochreiter\\'s diploma thesis (1991)[76] was called \"one of the most important documents in the history of machine learning\" by his supervisor Schmidhuber.[31] It not only tested the neural history compressor,[60] but also identified and analyzed the vanishing gradient problem.[76][77] Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM), published in 1997.[78] LSTM recurrent neural networks can learn \"very deep learning\" tasks[14] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins.[79] LSTM has become the  most cited neural network of the 20th century.[31]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content='In 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks.[80][81] 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.[82] This has become the most cited neural network of the 21st century.[31]\\nIn 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[83]', metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[84]\\nSince 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid[85] by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.\\nSimpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.\", metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'}),\n",
       " Document(page_content=\"Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[86][87][88] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[89] Key difficulties have been analyzed, including gradient diminishing[76] and weak temporal correlation structure in neural predictive models.[90][91] Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of\", metadata={'source': 'https://en.wikipedia.org/wiki/Deep_learning', 'title': 'Deep learning - Wikipedia', 'language': 'en'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retrevier_tool = create_retriever_tool(retrevier,\"deep learning\",\"search for information about deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep learning'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrevier_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
